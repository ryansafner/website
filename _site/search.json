[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Read my Statement of Teaching Philosophy\n\nHood College (2016–2023)\nCourse titles below link to the latest semester of each course website. Courses prior to Fall 2019 do not have a dedicated website, but slides and some assignments are posted here. Many courses may have prior website versions with full materials but are either password protected or answer keys are hidden. If you are interested in the answer keys/materials, I am happy to share them with you (just email me)!\n\n\n\n My Hextickers\n\n\n\n\n\n\nECMG 212 - Statistics for Economics and Business\n\n\n\nECON 205 - Principles of Macroeconomics\n\n\n\nECON 304 - International Political Economy\n\n\n\nECON 306 - Microeconomic Analysis\n\n\n\nECON 315 - Economics of the Law\n\n\n\nECON 316 - Game Theory\n\n\n\nECON 317 - Economics of Development\n\n\n\nECON 324 - International Trade\n\n\n\nECON 326 - Industrial Organization\n\n\n\nECON 410 - Public Economics\n\n\n\nECON 452 - History of Economic Thought\n\n\n\nECON 480 - Econometrics\n\n\n\nMGMT 399 - Internship for Business Administration\n\n\n\n\n\nWake Forest University (2015–2016)\n\n\n\nECN 150 - Introduction to Economics\n\n\nECN 272 - Austrian Economics\n\n\n\n\n\nGeorge Mason University (2013–2015)\n\n\n\nECON 385 - International Economic Policy\n\n\nECON 403 - Austrian Economics\n\n\nICES High School Summer Workshops"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Disclaimer: All opinions expressed in published (and unpublished) articles are my own (along with my coauthors, if applicable) and do not represent the views or policies of the U.S. Copyright Office or any other institution.\nPeer-Reviewed Journal Articles\n\nSafner, Ryan. 2024/forthcoming. “The Most Interesting Economist in History: Using an NCAA-Style Tournament Assignment to Teach the History of Economic Thought” Journal of Economics Teaching\n\n\n Published \n\n\nSafner, Ryan. (2023) “Honor Among Thieves: How 19th Century American Pirate Publishers Simulated Copyright Protection.” Economic Governance 24: 119-141\n\n\n Published   Preprint\n\n\n\nTarko, Vlad and Ryan Safner. (2022) “International Regulatory Diversity Over 50 Years: Political Entrepreneurship within Fiscal Constraints.” Public Choice 193(1-2): 79-108\n\n\n Published   Preprint\n\n\n\nSafner, Ryan. (2021) “‘Public Good,’ or ‘Good for the Public?’ Political Entrepreneurship and the Public Funding of Scientific Research.” Journal of Private Enterprise 36(1): 743-771\n\n\n Published   Preprint\n\n\n\nSafner, Ryan. (2016) “Institutional Entrepreneurship, Wikipedia, and the Opportunity of the Commons.” Journal of Institutional Economics 12(4): 743-771\n\n\n Published   Preprint\n\n\n\nSafner, Ryan. (2016) “The Perils of Copyright Regulation.” Review of Austrian Economics 29: 127-137\n\n\n Published   Preprint\n\n\nArticles Under Review\nGangotena, Santiago and Ryan Safner. (2022) “The Production of Increasing Returns: Physical Technology, Institutional Technology, and the Pitfalls of Production Functions.” (Revise & Resubmit requested)\n\n Preprint\n\n\nResearch in Progress\n“A Tale of Two Capitals: Modeling the Interaction between Ideas, Physical Capital, and Growth.” (with Santiago Gangotena)\nDistributing Patronage: Censorship, Freedom of the Press, and Copyright in the English Transition from Natural State to Open Access Order\nPirate Thy Neighbor: The Protectionist Roots of U.S. Recognition of Foreign Copyrights\nKickstart My Art: Is Crowdfunding a Substitute or a Complement to Intellectual Property Laws?\nCryptocopyright? The Prospects of Protecting Intellectual Property on the Blockchain\nThe Network Origins of Economic Growth (with Santiago Gangotena)"
  },
  {
    "objectID": "posts/visualizing-linear-regression-with-shiny.html",
    "href": "posts/visualizing-linear-regression-with-shiny.html",
    "title": "Visualizing Linear Regression with Shiny",
    "section": "",
    "text": "For my econometrics course this semester, I have been using R to help students visualize linear regression models. Running a regression in R is quite simple, as is intepretting the results, with a little bit of training. However, I emphasize that I want students to understand what is happening “inside the black box” of regression. I discourage blindly trusting R’s opaquely simple input and output, and get students to learn what R is doing under the hood, even if they will never have to manually estimate the model themselves.\n\nBrief Econometrics Review Incoming:\nOrdinary Least Squares (OLS) regression simply chooses the intercept (\\(\\hat{\\beta_0}\\)) and slope (\\(\\hat{\\beta_1}\\)) parameters for the equation of a line that best fits the data: \\[\\hat{Y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}X_i\\]\nby trying to minimize the Sum of Squared Errors (SSE). The error \\(\\epsilon\\) (or residual) of an observation is defined as the difference between the actual value of \\(Y\\) observed in the data associated with a given value of \\(X\\), and the predicted value, \\(\\hat{Y}\\) given \\(X\\):\n\\[\\hat{\\epsilon}=Y_i-\\hat{Y_i}\\]\nSo what OLS does is minimize the sum of the squared errors:\n\\[\\min \\sum^n_{i=1} \\hat{\\epsilon_i}^2\\]\nThis is a calculus problem that is solvable, if tedious. But at least it should be intuitive to understand what OLS does.\n\n\nVisualizing with Shiny\nBeyond grinding students down with pure theory, I wrote an interactive Shiny App with R that demonstrates this process to choosing the optimal slope and intercept.\nWith this example, I have a randomly-generated set of data (within specific parameters, to keep the graph scaled properly). I then allow users to choose a slope and intercept with sliders, to move the line accordingly. The graph also displays all of the errors as dashed red lines. Moving the slope or the intercept too much causes the generated line to create much larger errors. The graph also calculates the SSE (which again, OLS minimizes), in addition to the standard error of the regression (SER), which calculates the average size of the error.\nI would have loved to be able to graph the square of the errors, and display them, to show that OLS minimizes the area of those squares, if drawn. However, I was unable to figure out how to draw a square from each data point and the regression line (for squared error)| instead of a mere dashed line (for error).\nI love Shiny and have only begun to fully understand how it works and the benefits of its application. I have already coded several other models for intuitive visualization that I may post about later, namely a Calculus of Consent model of the optimal voting rule in public choice, and ambitiously, the consumer’s constrained optimization problem in intermediate microeconomics. Using sliders in shiny allow me to demonstrate the marginal effects of each parameter change on a model’s predicted outcome far better than just me proving several theoretical examples by way of equations. Students can instead see the change in real time."
  },
  {
    "objectID": "posts/replicating-statas-robust-option-for-ols-standard-errors-in-r.html",
    "href": "posts/replicating-statas-robust-option-for-ols-standard-errors-in-r.html",
    "title": "Replicating Stata’s ‘Robust’ Option for OLS Standard Errors in R",
    "section": "",
    "text": "One of the advantages of using Stata for linear regression is that it can automatically use heteroskedasticity-robust standard errors simply by adding , r to the end of any regression command. Anyone can more or less use robust standard errors and make more accurate inferences without even thinking about what they represent or how they are determined since it’s so easy just to add the letter r to any regression.\nIn R, robust standard errors are not “built in” to the base language. There are a few ways that I’ve discovered to try to replicate Stata’s “robust” command. None of them, unfortunately, are as simple as typing the letter r after a regression. Each has its ups and downs, but may serve different purposes.\nBelow, I will demonstrate the two methods, but first, let’s create some random data that will have heteroskedastic residuals.\nWe then run a regression of y on x:\nLet’s plot a scatterplot to visualize the data and add the regression line. Clearly, the data is “fan” shaped, centered on the regression line, but with larger and larger residuals (distance between the regression line and the data point, \\(\\hat{\\epsilon}_i=\\hat{Y}_i-Y_i\\)) as \\(X\\) gets larger.\nI have also broken up the scatterplot into 5 different sections over the range of x values. Below, I plot density plots of the residuals over each of the 5 different ranges of x values, and we can clearly see that the variance of the residuals dramatically increases as x increases.\nUsing the lmtest package, we can also formally run a Breusch-Pagan test for heteroskedasticity."
  },
  {
    "objectID": "posts/replicating-statas-robust-option-for-ols-standard-errors-in-r.html#method-1-sandwich-package",
    "href": "posts/replicating-statas-robust-option-for-ols-standard-errors-in-r.html#method-1-sandwich-package",
    "title": "Replicating Stata’s ‘Robust’ Option for OLS Standard Errors in R",
    "section": "Method 1: Sandwich package",
    "text": "Method 1: Sandwich package\nIn order to understand what the “fix” in this method is actually doing, we also need to look “under the hood” of what R is doing when it runs OLS and stores everything in the lm regression object.\nOne thing stored in reg is the variance-covariance matrix, estimating the covariance of each OLS estimator (the “betas”) with every other OLS estimator:\n\\[ \\begin{pmatrix} cov(\\hat{\\beta_0},\\hat{\\beta_0}) & cov(\\hat{\\beta_0},\\hat{\\beta_1}) & \\cdots & cov(\\hat{\\beta_0},\\hat{\\beta_k})\\\\\\\\\\\\ cov(\\hat{\\beta_1},\\hat{\\beta_0}) & cov(\\hat{\\beta_1},\\hat{\\beta_1}) & \\cdots & cov(\\hat{\\beta_1},\\hat{\\beta_k})\\\\\\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\\\\\ cov(\\hat{\\beta_k},\\hat{\\beta_0}) & cov(\\hat{\\beta_k},\\hat{\\beta_1}) & \\cdots & cov(\\hat{\\beta_k},\\hat{\\beta_k})\\\\\\\\\\\\ \\end{pmatrix}\\]\nSince the covariance of anything with itself is the variance, the diagonal elements of this matrix are the variances of the OLS estimators:\n\\[\\begin{pmatrix}var(\\hat{\\beta_0}) & cov(\\hat{\\beta_0},\\hat{\\beta_1}) & \\cdots & cov(\\hat{\\beta_0},\\hat{\\beta_k})\\\\\\\\\\\\ cov(\\hat{\\beta_1}, \\hat{\\beta_0}) & var(\\hat{\\beta_1}) & \\cdots & cov(\\hat{\\beta_1},\\hat{\\beta_k})\\\\\\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\\\\\ cov(\\hat{\\beta_k},\\hat{\\beta_0}) & cov(\\hat{\\beta_k},\\hat{\\beta_1}) & \\cdots & var(\\hat{\\beta_k})\\\\\\\\\\\\ \\end{pmatrix}\\]\nSo if we look at the simple \\(2 \\times 2\\) variance-covariance matrix in our simple reg using vcov, we see.\nvcov(reg)\n##             (Intercept)            x\n## (Intercept)  0.27315741 -0.039976551\n## x           -0.03997655  0.007897029\nWe can extract just the diagonal of the matrix with diag():\ndiag(vcov(reg))\n## (Intercept)           x \n## 0.273157410 0.007897029\nThese are the variances of \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\). Since the standard error of an estimator is the square root of its variance, we simply square root these values to get the standard errors of \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\), which were originally reported in our regression output next to the coefficient estimates.\nsqrt(diag(vcov(reg)))\n## (Intercept)           x \n##  0.52264463  0.08886523\nNow, the whole problem is we know that due to heteroskedasticity, the standard errors are incorrectly estimated. To fix this, we use the sandwich package that allows us to manually recalculate the variance-covariance matrix using methods robust to heteroskedasticity. This is why I went through the trouble of describing the variance-covariance matrix above, as we will be recalculating it using a different method, the HC1 method, which is how Stata calculates it. I then store these calculates as rse in my original lm object called reg.\nlibrary(\"sandwich\") # package that allows for robust SE estimation\n# create Robust Standard Errors for regression as 'reg$rse'\nreg$rse &lt;-sqrt(diag(vcovHC(reg, type=\"HC1\")))\n# same procedure as above but now we generate vcov with \"HC1\" method\nIf we now want to recreate the regression output table produced by summary(reg), we need to use the coeftest function, which is a part of the lmtest package. Just to verify, if we run coeftest() on our original reg, it prints the regression output table with coefficient estimates, standard errors, \\(t\\)-statistics, and \\(p\\)-values.\ncoeftest(reg) # test with normal SEs\n## \n## t test of coefficients:\n## \n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 0.685077   0.522645  1.3108   0.1905    \n## x           0.764836   0.088865  8.6067   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nIf we run it again, but set the vcov option to ccovHC(reg, \"HC1\"), it will print the robust standard errors.\ncoeftest(reg,vcov=vcovHC(reg,\"HC1\")) # tests with robust SEs\n## \n## t test of coefficients:\n## \n##             Estimate Std. Error t value  Pr(&gt;|t|)    \n## (Intercept) 0.685077   0.312060  2.1953    0.0286 *  \n## x           0.764836   0.093579  8.1732 2.504e-15 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nThese command simply print the robust standard errors for us to see in the console as we run our analyses. If we want to take these and actually output them in a presentable regression table, we will use the well-known stargazer package, used to take R regression lm objects and print scholarly journal-quality regression tables.\nThe nice thing is stargazer has an option to set where the standard errors are pulled from. We stored our robust standard errors in reg as a vector called rse. Below, I print the stargazer regression table (with several personalized options) for this webpage, showing the our regression twice, once with the normal standard errors, and the second time with the robust standard errors. For more, see Jake Russ’ cheat sheet.\nlibrary(\"stargazer\")\nstargazer(reg, reg, \n          se=list(NULL,reg$rse), \n          type=\"html\",\n          column.labels = c(\"Normal\", \"Robust SEs\"), \n          title=\"Regression Results\", \n          dep.var.caption = \"\",\n          omit.stat=c(\"adj.rsq\",\"f\")) \nThe key to notice is se=list(NULL,reg$rse), which creates a list of objects from which to pull the standard errors for each regression in the table. The first regression uses the standard methods, needing no special source, so it is set to NULL. The second regression, also reg, uses our robust standard errors stored in reg$rse. The output of the table is below:\n\nRegression Results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\n\n\nConstant\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\nR2\n\n\nResidual Std. Error (df = 498)\n\n\n\n\n\nNote:\n\n\n\nA casual search around the internet, as well as the textbook that I use shows that this is the most common or reccomended method for achieving robust standard errors."
  },
  {
    "objectID": "posts/replicating-statas-robust-option-for-ols-standard-errors-in-r.html#method-2-using-estimatr",
    "href": "posts/replicating-statas-robust-option-for-ols-standard-errors-in-r.html#method-2-using-estimatr",
    "title": "Replicating Stata’s ‘Robust’ Option for OLS Standard Errors in R",
    "section": "Method 2: Using estimatr",
    "text": "Method 2: Using estimatr\nI recently discovered another package called estimatr that achieves the simplicity of changing a single word, just like in Stata.\nLoading the estimatr package, all we need to do is create a new regression (I’ll call it reg.robust) and instead of running a normal linear model with lm, we run lm_robust, and set the standard errors se_type=\"stata\" to calculate using the HC1 method (same as above).\nlibrary(\"estimatr\")\nreg.robust&lt;-lm_robust(y~x,se_type = \"stata\")\nsummary(reg.robust)\n## \n## Call:\n## lm_robust(formula = y ~ x, se_type = \"stata\")\n## \n## Standard error type:  HC1 \n## \n## Coefficients:\n##             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper  DF\n## (Intercept)   0.6851    0.31206   2.195 2.860e-02  0.07196   1.2982 498\n## x             0.7648    0.09358   8.173 2.504e-15  0.58098   0.9487 498\n## \n## Multiple R-squared:  0.1295 ,    Adjusted R-squared:  0.1277 \n## F-statistic:  66.8 on 1 and 498 DF,  p-value: 2.504e-15\nWe can see the standard errors are now identical to the robust ones from the method above.\nOne other nicety of estimatr is that it can create tidy data.frame versions of R default regression output tables, much like the broom package in the tidyverse. We do this simply with the tidy() command on our reg.robust.\ntidy(reg.robust)\n\n\n\nUntil today, I thought that was all estimatr could do: it could show us the robust standard errors, but we could not present it in an output table with stargazer. lm_robust objects do not get along well with stargazer, only lm objects.\nDocumentation is extremely scarce, but there is a starprep() command to enable use of estimatr lm_robust objects with stargazer. After a lot of searching and trial and error, the process seems to be that using starprep extracts only the (robust) standard errors from the lm_robust regression, meaning we just need to insert this into stargazer’s se= option.\n# this is what starprep extracts\nstarprep(reg.robust)\n## [[1]]\n## (Intercept)           x \n##  0.31205969  0.09357893\nBelow, again, I run stargazer on our original reg twice, with the second instance using robust standard errors via estimatr. Specifically notice the list for se; again, like the fist method above, we use the default for the first regression (hence NULL), and for the second, we use starprep(reg.robust) to extract from estimatr.\nstargazer(reg, reg, \n          se=starprep(reg,reg.robust), \n          type=\"html\",\n          column.labels = c(\"Normal\", \"Robust SEs\"), \n          title=\"Regression Results\", \n          dep.var.caption = \"\",\n          omit.stat=c(\"adj.rsq\",\"f\")) \n\nRegression Results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\n\n\nConstant\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\nR2\n\n\nResidual Std. Error (df = 498)\n\n\n\n\n\nNote:\n\n\n\nAgain, we can see both methods achieve results identical to Stata. The nice thing about estimatr is we do not need to mess around with the variance-covariance matrix!"
  },
  {
    "objectID": "posts/if-youre-going-to-learn-r-learn-the-tidyverse.html",
    "href": "posts/if-youre-going-to-learn-r-learn-the-tidyverse.html",
    "title": "If You’re Going to Learn R, Learn the Tidyverse",
    "section": "",
    "text": "This is an opinionated post based on how I teach my undergraduate econometrics course. It will not be for everybody. The title applies mostly to anyone who wants to do data science or econometrics with R. This is the second time I have taught this course with R, and I have changed it around in many ways that I think optimize the process for students. In this post, I’ll cover just two major changes:\nThe last time I taught this course (and my first exploration with R), I did neither. Part of this was because I had just learned base R a few months before the course began, and was still learning and discovering new commands as the semester went on. I waited several weeks to introduce R, starting instead with econometric theory and review, and then interspersing bits of R as it became relevant for each task (running regressions, making plots, changing variables, and various different models). While it was fine for most students, there are some changes that will make students’ lives easier.\nI am still partway through the course, so it remains to be seen if students R skills are more developed by the end of the course (as they wrap up their projects) because they learned a lot of R first, versus in bite-sized chunks spread out over the semester."
  },
  {
    "objectID": "posts/if-youre-going-to-learn-r-learn-the-tidyverse.html#tidyverse-and-opinionated-r",
    "href": "posts/if-youre-going-to-learn-r-learn-the-tidyverse.html#tidyverse-and-opinionated-r",
    "title": "If You’re Going to Learn R, Learn the Tidyverse",
    "section": "tidyverse and Opinionated R",
    "text": "tidyverse and Opinionated R\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. - tidyverse.org\n\nOne of the common refrains about what is awesome/terrible about R is that there are multiple ways to accomplish the same task. Here is where the opinions begin, so I’ll give mine: Code written with tidyverse packages simply looks a lot better and is far easier for humans to read, particularly if you follow the style guidelines, as I do.\nFor example, the following code takes data from the excellent gapminder dataset and package, and subsets the data to:\n\nlook only at U.S. observations\nkeep only the year, gdpPercap and pop variables\ncreate a new variable called GDP by multiplying gdpPercap and pop\n\n\n\nCode\ngapminder1&lt;-gapminder[gapminder$country==\"United States\", c(\"year\", \"gdpPercap\", \"pop\")]\n\ngapminder1$gdp&lt;-gapminder1$gdpPercap*gapminder1$pop\n\n\nThis is doable in base R, and often requires saving the output as a new object for later use.\nBelow, the same procedure is done with dplyr and using the pipe %&gt;% from magrittr (both part of the tidyverse).\n\n\nCode\ngapminder %&gt;%\n  filter(country == \"United States\") %&gt;%\n  select(year, gdpPercap, pop) %&gt;%\n  mutate(GDP=gdpPercap * pop)\n\n\n# A tibble: 12 × 4\n    year gdpPercap       pop     GDP\n   &lt;int&gt;     &lt;dbl&gt;     &lt;int&gt;   &lt;dbl&gt;\n 1  1952    13990. 157553000 2.20e12\n 2  1957    14847. 171984000 2.55e12\n 3  1962    16173. 186538000 3.02e12\n 4  1967    19530. 198712000 3.88e12\n 5  1972    21806. 209896000 4.58e12\n 6  1977    24073. 220239000 5.30e12\n 7  1982    25010. 232187835 5.81e12\n 8  1987    29884. 242803533 7.26e12\n 9  1992    32004. 256894189 8.22e12\n10  1997    35767. 272911760 9.76e12\n11  2002    39097. 287675526 1.12e13\n12  2007    42952. 301139947 1.29e13\n\n\nThe beauty of tidyverse (particularly dplyr, which will be used the most for data wrangling) comes from a few features:\n\nIt uses active, common sense, natural language verbs to accomplish most of its tasks. filter, select, and mutate (among others) are easy to understand what is happening at each stage.\nIt allows use of the pipe %&gt;% to chain commands into a single sequence (and better yet, every time I use a pipe, I start a new line to make code far more readable).\nIt shows you the output by default and does not store (or overwrite) it anywhere until or unless you assign it to an object. This allows you to preview what your code does before you need to worry about saving it.\n\nNote that this can be done without use of the pipe, and by storing objects, as such:\n\n\nCode\ngapminder_US&lt;-filter(gapminder, country == \"United States\")\ngapminder_US&lt;-select(gapminder_US, year, gdpPercap, pop)\ngapminder_US&lt;-mutate(gapminder_US, GDP=gdpPercap * pop)\ngapminder_US\n\n\n# A tibble: 12 × 4\n    year gdpPercap       pop     GDP\n   &lt;int&gt;     &lt;dbl&gt;     &lt;int&gt;   &lt;dbl&gt;\n 1  1952    13990. 157553000 2.20e12\n 2  1957    14847. 171984000 2.55e12\n 3  1962    16173. 186538000 3.02e12\n 4  1967    19530. 198712000 3.88e12\n 5  1972    21806. 209896000 4.58e12\n 6  1977    24073. 220239000 5.30e12\n 7  1982    25010. 232187835 5.81e12\n 8  1987    29884. 242803533 7.26e12\n 9  1992    32004. 256894189 8.22e12\n10  1997    35767. 272911760 9.76e12\n11  2002    39097. 287675526 1.12e13\n12  2007    42952. 301139947 1.29e13\n\n\nI am not alone in this view. As far as it relates to teaching, the implications are clear: The overwhelming majority of students are new to “programming”, so they will be frustrated regardless of what order the content was taught, or in what flavor of language. They do not need to know the “base R” way of doing something just so that they can see that the tidyverse may be better or more efficient - they just need to learn one way to accomplish their task, it might as well be (what I think is) the “better” one.\nSo, I began the course (after two days of overview, why this course is important, useful, etc.) with 4 intensive classes of learning R, and tidyverse specifically. First, a day about base R, second, a day about ggplot2 for data visualization, third, a day of data wrangling with tidyverse, and finally a day about workflow and other tools (mainly r markdown). I cover the basics behind each of them, and what I have learned, below:"
  },
  {
    "objectID": "posts/if-youre-going-to-learn-r-learn-the-tidyverse.html#class-1-is-base-r-necessary",
    "href": "posts/if-youre-going-to-learn-r-learn-the-tidyverse.html#class-1-is-base-r-necessary",
    "title": "If You’re Going to Learn R, Learn the Tidyverse",
    "section": "Class 1: Is Base R Necessary?",
    "text": "Class 1: Is Base R Necessary?\nIt somehow seems more “pure” to teach R from the ground up: First we discuss basic R commands, then we build more complicated functions in, then we show how to manipulate data, then we show how to plot, and later: “oh by the way there are these packages that do all of this more elegantly and in half as many lines of code.” That is how I taught econometrics last year.\nWe need to remember this is a class in econometrics and data analysis that uses R, not a class in computer science or the R language. In fact, people with a computer science/programming background seem to find R particularly annoying as a programming language. It is highly domain-specific (that domain chiefly being statistics), and should be appreciated as such.\nIn any case, I still made a point this year to make my first R-based class entirely about Base R without the bells and whistles.1 Students and R-users need to understand some basic syntax of functions, as well as the object-oriented nature of the language. I made sure that they understand the following really well:\n\nWhat are packages, how to find, install, and load them.\nHow to get help in R and on the internet for R functions.\nDifferent types of objects: especially vector and data.frame\nDifferent data classes: especially numeric and character, and how to check them\nSome basic functions for making vectors and for getting statistics (c(), mean(), sd(), etc.)\nThe basics of data frames: recognizing each column is a vector, how to summarize them, how to subset by row, column, and element\n\nI try not to go overboard (I omit things like factor and logical classes, list or matrix objects) and tell them not to worry too much about manipulating dataframes in Base R, as that is what tidyverse will accomplish much more intuitively and efficiently."
  },
  {
    "objectID": "posts/if-youre-going-to-learn-r-learn-the-tidyverse.html#class-2-data-visualization-with-ggplot2",
    "href": "posts/if-youre-going-to-learn-r-learn-the-tidyverse.html#class-2-data-visualization-with-ggplot2",
    "title": "If You’re Going to Learn R, Learn the Tidyverse",
    "section": "Class 2: Data Visualization with ggplot2",
    "text": "Class 2: Data Visualization with ggplot2\nThe second R-class I taught students all about data visualization with ggplot2. Not only do I think it is aesthetically superior to anything from Base R,2 it also allows students to think about the important elements of a plot, and optimize each one accordingly, with the “grammar of graphics.” This is a pretty steep learning curve compared to typing plot(y,x) and seeing a scatterplot appear, but in the end, it is worth it.\nIn class, I build a few plots layer by layer:\n\ndata\naesthetics\ngeometric objects\nfacets\nlabels\nscales\ntheme\n\nAt each layer, I explain what each layer does and many of the possibilities for each layer.\n\n\nCode\nggplot(data = mpg)+\n  aes(x = displ,\n        y = hwy)+\n  geom_point(aes(color = class))+\n  geom_smooth()+\n  facet_wrap(~year)+\n  labs(x = \"Engine Displacement (Liters)\",\n       y = \"Highway MPG\",\n       title = \"Car Mileage and Displacement\",\n       subtitle = \"More Displacement Lowers Highway MPG\",\n       caption = \"Source: EPA\",\n       color = \"Vehicle Class\")+\n  scale_color_viridis_d()+\n  theme_minimal()+\n  theme(text = element_text(family = \"Fira Sans\"),\n        legend.position=\"bottom\")\n\n\n\n\n\n\n\n\n\nThis is also the students’ first exposure to tidyverse, though it is not yet apparent. My one regret, in retrospect, is that plot layers are combined with + instead of %&gt;%.3 After learning other tidyverse packages such as dplyr, students would try to add plot layers with %&gt;% but I would continuously have to remind them that layers are combined with +.\nRecall, all of this is done well before we cover basic statistics or econometric theory. When I was teaching students how to construct various plots with ggplot2, this was before they knew why they needed a scatterplot or a boxplot."
  },
  {
    "objectID": "posts/if-youre-going-to-learn-r-learn-the-tidyverse.html#class-3-data-wrangling-with-tidyverse-mostly-dplyr",
    "href": "posts/if-youre-going-to-learn-r-learn-the-tidyverse.html#class-3-data-wrangling-with-tidyverse-mostly-dplyr",
    "title": "If You’re Going to Learn R, Learn the Tidyverse",
    "section": "Class 3: Data Wrangling with tidyverse (mostly dplyr)",
    "text": "Class 3: Data Wrangling with tidyverse (mostly dplyr)\nThe third class was all about tidyverse as a unifying set of packages with a common philosophy and grammar. I did discuss several core packages:\n\ntibble for friendlier dataframes4\nmagrittr for using the pipe %&gt;% to chain code together\nreadr for importing data (mostly .csv5\ntidyr for reshaping data\n\nBut the primary focus was on dplyr and its verbs:\n\nfilter() to keep selected observations\nselect() to keep selected variables\narrange() to reorder observations by a value\nmutate() to create new variables\nsummarize() to create summary statistics\ngroup_by() for performing operations by group\n\nWe worked with gapminder data to accomplish a wide variety of tasks using these commands."
  },
  {
    "objectID": "posts/if-youre-going-to-learn-r-learn-the-tidyverse.html#class-4-optimizing-workflow-r-projects-and-markdown",
    "href": "posts/if-youre-going-to-learn-r-learn-the-tidyverse.html#class-4-optimizing-workflow-r-projects-and-markdown",
    "title": "If You’re Going to Learn R, Learn the Tidyverse",
    "section": "Class 4: Optimizing Workflow: R Projects and Markdown",
    "text": "Class 4: Optimizing Workflow: R Projects and Markdown\nThe fourth and final class dedicated to R was all about optimizing workflow with a few tweaks. I have not used Microsoft Office products in about 10 years (more posts to come later), and in the last year, have migrated ALL of my document-preparation (that’s research papers, teaching materials, exams, slides, websites, everything) to markdown files I write inside of R Studio and track with version control on GitHub.\nWhen I show to students that there are other options to Microsoft Word and Powerpoint, their jaws drop, particularly when I show all that you can do with a single plain-text document (integrating text, code, commands, plots, citations and bibliography etc) that exports to pdf, html, slides, and other output. Last semester, after I showed them R Markdown, several students told me it was the best thing about the class, and some still use it for other assignments in other classes on their own. This deserves its own series of posts, so for now I will focus on the two or three things I tried to teach students in addition to how to use Markdown.\nFirst, R Projects are absolutely essential. I discovered these late in the game last year, but now realized that they solve far more problems than trying to do without them.\nThe #1 unnecessary problem I encounter with students is trying to load data from external sources. The world is not full of tidy pre-cleaned data, or even data that only come in .csv formats. tidyr is great for that, and so is readr, but the problem actually is one of basic file operations on a computer: students (and R) don’t know where the data is saved on their computers! Rather than trying to teach them how to write out relative or absolute file paths to locate files on their computer, R Projects solve this problem by setting the default working directory to the folder where the project is stored on their computer. That means that if you store the data file in that folder, you only need to load it (with readr or equivalent) with (e.g.) read_csv(\"thedata.csv\"), no more worrying about file paths!\nI also encourage students to create a logical folder hierarchy within their projects, similar to what I show in this repository.\nSecond, version control with Github. I never actually got around to showing this in class, but a number of students expressed interest in learning how to do this. I feel it’s a bit advanced and requires a bit more computing expertise (but not too much, since I’m able to pull it off!), but I use it constantly. Perhaps more posts on this later.\nIn any case, after teaching these workflow methods, looking back on the first 3 R classes, I am tempted to just start from scratch with projects and markdown and make students use them from the beginning. Perhaps next year."
  },
  {
    "objectID": "posts/if-youre-going-to-learn-r-learn-the-tidyverse.html#footnotes",
    "href": "posts/if-youre-going-to-learn-r-learn-the-tidyverse.html#footnotes",
    "title": "If You’re Going to Learn R, Learn the Tidyverse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough I allude to more complex examples to give them a taste of what’s to come!↩︎\nIf it was not clear in my posts by now, I have high aesthetic standards.↩︎\nI believe this is due to the unique history of ggplot2 coming before the tidyverse was a full idea. Though I hear in future versions, this may be fixed!↩︎\nI simply replaced all dataframes in the course with tibbles↩︎\nAnd derivative packages such as readxl and haven for importing other types of data such as .xlsx or Stata’s .dat.↩︎"
  },
  {
    "objectID": "posts/econometrics-data-science-and-causal-inference.html",
    "href": "posts/econometrics-data-science-and-causal-inference.html",
    "title": "Econometrics, Data Science, and Causal Inference",
    "section": "",
    "text": "This summer, I am overhauling my econometrics class in many ways, in part because I was pleased to recieve a teaching grant from my college to make more R resources for my econometrics class. Last Fall was the first time I had taught it using R, and I’ve learned a ton since then. Expect a flurry of posts in the coming weeks more on those topics.\nThis post, however, explores some of the trends that I have been thinking about in teaching econometrics, and something monotonous that I have been struggling with that encapsulates the tension in these trends: what to name my course. I of course need to officially stick to the Procrustean Bed of the title in my college’s course catalog: ECON 480 - Econometrics, but in more causal conversation, or perhaps as a course subtitle, I have been torn between “data science” and “causal inference.” This class is an introduction to econometrics, and assumes students have taken basic statistics, so this “problem” is really just marketing on my part — it’s just a run-of-the-mill introductory econometrics course, but I like to add in a few bells and whistles!\nAfter thinking on this more, it seems to me there that we might be able to delineate a few possible directions that our field is heading. I hasten to qualify that I am thinking about this much more as a teacher, of these topics, less as a practitioner, as my own research niche is applied economic theory, albeit with a steadily-growing fascination with these methods. Nor am I a historian of econometric thought.\nThese two trends or approaches I’ll call “data science” and “causal inference.” For the purposes of wrestling with this post, I will examine each of these forward-looking trends separately. I don’t want to rule out the possibility at all that these are entirely complementary, and I am making a big deal out of nothing. Nonetheless, I think it is an interesting distinction to examine."
  },
  {
    "objectID": "posts/econometrics-data-science-and-causal-inference.html#data-science",
    "href": "posts/econometrics-data-science-and-causal-inference.html#data-science",
    "title": "Econometrics, Data Science, and Causal Inference",
    "section": "“Data Science”",
    "text": "“Data Science”\n“Data science” is the hip new catchall term for using statistical software to analyze data for a variety of purposes. It is commonly interpreted as the intersection of statistics, computer science, and domain expertise (e.g. business, biology, etc). I personally like the definition offered by the following tweet:\n{{&lt; tweet 198093512149958656 &gt;}}\nThe modal “data scientist” seems to be a young practitioner in industry or academia that use programming languages like R and Python to wrangle, visualize, and present data to solve empirical problems in their domain of expertise. On the academic/teaching side, self-styled “data scientists” seem to be trendy statisticians, computer scientists, or biologists (e.g. “bioinformatics”). New masters degrees and certifications in “Data Science” or “Data Analytics” seem to be popping up left and right at major universities.\nA large part of me wants to ride the coattails of this cool new trend and call/subtitle my class something like “Introduction to Data Science.” I decided against it for a few reasons that I think are instructive.\nFirst, I would be encroaching the turf of my colleagues in Statistics and Computer Science departments that often have courses with explicit titles like this. These classes are often a rehash of classic introductory statistics classes (probability, the normal distribution, \\(t\\)-tests, the central limit theorem, etc) but where software and simulation tend to replace pure theory and statistical proofs in a much more student-friendly way. It is a lot more intuitive and less tedious to understand a \\(p\\)-value via programming commands or simulating a null distribution of 10,000 bootstrapped samples than to learn the theoretical Student’s \\(t\\)-distributions and look up critical values from a table (as I once had to!).\nSecond, although some econometrics textbooks do teach introductory econometrics this way1, a good econometrics class, in my opinion, is much more than just an introductory statistics class (first review probability, distributions, statistical estimators, hypothesis testing, linear regression) that finally bleeds into the models that empirical econometricians actually — oh no, the semester’s already over!\nEconometrics has a particularly opinionated view of statistical models, and often uses them to a very different end than most “data science”-type uses. This is precisely what distinguishes uses of statistics in economics from its use in other domain areas2, the focus on the second major trend I discuss below, causal inference.\nPerhaps the paradigmatic application of “data science” beyond mere descriptive statistics and causal inference is “machine learning” and associated terms and techniques (e.g. “big data,” “neural networks,” “artificial intelligence,” etc). These issues often deal with massive “wide” datasets, frequently with more variables than observations. Take a website such as Facebook, which has a ton of information about a user – locations they visit the website from, who their friends are, what pages they have interacted with, etc. Machine learning techniques such as neural networks, as I understand them, try to identify what characteristics predict a certain outcome. Facial recognition software seems to me to operate similarly: take a large number of pre-identified images (e.g. a human at some point determined the image indeed contained a face) for training an algorithm, pass those images through \\(n\\)-number of layers to detect common characteristics of “faces” vs. “not-faces”, and then additional subtleties that differentiate one face from another, and after \\(n\\)-layers and \\(m\\) features are determined (where both \\(n\\) and \\(m\\) are massive numbers!), then the algorithm can take a new un-classified image and determine if it includes a face, or, if it is a very good algorithm, whose face it is. But, as this excellent video reiterates, nobody understands how the neural network that successfully identifies faces works.\nThe key feature here is that it these technqiues are atheoretical, they are entirely empirical and judged on their rate of success. The algorithm itself has no underlying theory behind what “causes” a face or determines a face, only a purely empirical process of identifying patterns statistically by brute force. Nobody actually knows, or perhaps even could know, why or how an algorithm works."
  },
  {
    "objectID": "posts/econometrics-data-science-and-causal-inference.html#causal-inference",
    "href": "posts/econometrics-data-science-and-causal-inference.html#causal-inference",
    "title": "Econometrics, Data Science, and Causal Inference",
    "section": "“Causal Inference”",
    "text": "“Causal Inference”\nCausal inference, by contrast, whatever some hardcore practitioners might say about merely “letting the data speak for itself”, necessitates an underlying theory to explain a relationship or a process.\nWithout going into a full post on methodology, economics is a theory-driven discipline in a way like few others. This is why I expect that “big data” and “machine learning” techniques will offer only marginal insights in economic research relative to the tried-and-true methods (and I am in good company). It is certainly true that some theories may be verified or disproven by data, or entirely new theories emerge via analyzing data that may never have otherwise been discovered. However, the main tool of empirical work is to identify and measure causal effects already theorized or hypothesized from solid economic theory.3 Data can never “speak for itself,” because we wouldn’t know what data is worth collecting in the first place. I like to think about it with the (attributed) Immanuel Kant quote:\n\nExperience without theory is blind, while theory without experience is mere intellectual play.\n\nEconomists and econometricians have always emphasized the role of causal inference in econometrics. I recall learning - to my surprise - as an undergraduate and as a graduate in my courses that with clever identification strategies (instrumental variables, regression discontinuity, differences-in-differences, fixed effects, etc), we can do a lot more than just say things are “correlated,” and thus gore the sacred refrain of statistics professors everywhere (“correlation does not imply causation”). I never experienced what learning econometrics was like in the 1980s or 1990s, but this view has only grown stronger with the “credibility revolution” described by Angrist and Pischke (2010, 4):\n\nEmpirical microeconomics has experienced a credibility revolution, with a consequent increase in policy relevance and scientific impact…[A]s we see it, the primary engine driving improvement has been a focus on the quality of empirical research designs.\n\nHaving much more omnipresent and higher-quality micro-level data has also helped.\nHowever, I have noticed in recent years in a particular strand of econometrics teaching has gotten more pathological about causal inference (I don’t mean to use that term pejoratively!). Not all of the work in this trend is by economists, but it seems to finally be seeping into econometrics pedagogy.\nI think the quintessential tools for this new wave of teaching causal inference are the Directed Acyclic Graphs (DAGs) popularized by famed computer scientist Judea Pearl, or the “do-calculus” for more advanced treatments. I see this now showing up in excellent teaching materials like Scott Cunningham’s Causal Inference: the Mixtape, this great but technical blog post by Michael Nielson, a lecture in Andrew Heiss’ class, and Nick Huntington-Klein’s excellent lecture slides for his course in Economics, Causality, and Analytics (now there’s a good title!). More and more people also seem to be linking to the great daggity.net tool for drawing and interpreting DAGs.\nAfter reading Pearl and Mackenzie (2018)’s The Book of Why, I briefly felt much more zealous about claiming that we can indeed argue “X does cause Y, dammit!”. The review of DAGs by Imbens (2019) brought me back down to earth. DAGs are a great pedagogical tool4 but aren’t the silver bullet for clarifying empirical work. Imbens makes a compelling case that DAGs have very little practicality beyond”toy models” such as those in Pearl and Mackenzie (2018) and standard econometric research designs since the “credibility revolution” (Angrist and Pischke, 2010) work far better in practice. For the moment, that’s fine for me — I may make a post later about the benefits5 — I think my students will find them very useful. I am more convinced with Pearl’s argument that causal models are a necessary ingredient to the progress of artificial intelligence."
  },
  {
    "objectID": "posts/econometrics-data-science-and-causal-inference.html#a-compromise",
    "href": "posts/econometrics-data-science-and-causal-inference.html#a-compromise",
    "title": "Econometrics, Data Science, and Causal Inference",
    "section": "A Compromise",
    "text": "A Compromise\nIn any case, thinking about these two strands makes the development of the field seem richer and more dynamic. I again add that I am not trying to artificially carve out an antagonism between these two approaches, in fact I hope to marry them into the latest iteration of my econometrics course. One of the appeals I try to make to my students (some of whom are Economics majors, some Business majors, and a mixture of others such as Biology or Environmental Science) is that the tools we learn in this class will not only make you better consumers of science and studies, but also may be invaluable in the emerging job market. “Data science” again is one of the sexy trends of the 2010s and beyond. Learning how to import, wrangle, analyze, and present data with statistical software and programming languages like R already makes students budding “data scientists.” Futhermore, causal inference with empirical models and research designs is the economist’s comparative advantage that will set economics majors apart from your average “data science” or “data analytics” certificate-holder."
  },
  {
    "objectID": "posts/econometrics-data-science-and-causal-inference.html#footnotes",
    "href": "posts/econometrics-data-science-and-causal-inference.html#footnotes",
    "title": "Econometrics, Data Science, and Causal Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStock and Watson, for example, which I taught with for 2 years↩︎\nEpidemiology is perhaps closest to economics in this regard.↩︎\nI don’t want to take the hardcore position here that there is no validity to empirical work in economics. Sometimes multiple economic principles might be operating in combination or against each other through multiple channels in the real world - empirical work helps us to tease them out in a useful way.↩︎\nThere are limitations to them, such as the inability to add cutoff variables for regression discontinuity design models, or interaction terms. Huntington-Klein seems to fudge this by creating nodes like \\(X&lt;c\\), or \\(X * Z\\), for the respective problems mentioned)↩︎\nNamely, 1. making model assumptions explicit, 2. using proper correlations in the data to falsify a proposed causal model, and 3. showing that there are not all variables need to be controlled for, indeed controlling for certain types of variables actually introduces bias!↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ryan Safner",
    "section": "",
    "text": "I am an Economist at the Office of the Chief Economist at the U.S. Copyright Office.\nPreviously, I was Associate Professor of Economics at Hood College, and I have taught 17 different economics courses over the last 10 years. I am a firm believer of providing public goods & promoting open source materials, so you can find all course materials on my custom course websites.\nMy personal research explores the political economy of innovation, technological growth, and intellectual property using the tools of new institutional economics, public choice, and market process economics. I write about these issues in a non-technical way at my Substack, Safnerism.\nI am a passionate user of R for data analysis, and Quarto and GitHub for writing reproducible research, using version control, and managing my workflow. I keep a repository of information and tips about using these tools, as well as write about data science and economics at safneR scripts.\n\n\n\n\n\nDisclaimer: All opinions expressed on this website are my own and do not represent the views or policies of the U.S. Copyright Office or any other institution."
  },
  {
    "objectID": "courses/econ-304.html",
    "href": "courses/econ-304.html",
    "title": "ECON 304 — International Political Economy",
    "section": "",
    "text": "[Spring 2017 Syllabus]\n\nThe curious task of economics is to demonstrate to men how little they really know about what they imagine they can design. - F.A. Hayek, 1974 Economics Nobel Laureate\n\nThis course examines the dual role of political institutions and economic forces that determine the relative prosperity, conflict, and/or flourishing of human beings in different social settings. As political economy–the enduring tradition that Adam Smith and others developed before technical “economics” emerged in the 20th century–it deals with some of the moral and institutional problems that societies must grapple with, and why some succeed or fail more relative to others. While we consider the theory of various political and economic institutions, we often apply them to an international context of trade, globalization, foreign aid, conflict, development in our increasingly global-oriented cultures. As a result, a major goal of this course is understanding the state of the world and current events today, as well as being able to reason through political economic problems.\nThis is an economics course, and as such we use the tools of microeconomics to understand not just economic, but also political, social, and cultural issues. Basic familiarity with microeconomics is essential to understanding how decisions made by agents in different institutional contexts lead to social coordination and the creation of value (or fail to do so). While I would prefer that you have some background, it is possible to take the course with little to no understanding of formal economic theory. The required prerequisite courses are ECON 205, PCSI 215, OR GLBS 200.\nThis course will have been successful if you are able to comprehend and apply the tools and insights of political economy towards explaining differences in real world political, economic, and cultural settings and are able to analyze current events. Primarily, this means applying models of rational choices for individual and firm behavior in different social contexts, and understanding the role of market institutions of property rights, prices, and profit and losses in bringing about social coordination."
  },
  {
    "objectID": "courses/econ-304.html#overview",
    "href": "courses/econ-304.html#overview",
    "title": "ECON 304 — International Political Economy",
    "section": "",
    "text": "[Spring 2017 Syllabus]\n\nThe curious task of economics is to demonstrate to men how little they really know about what they imagine they can design. - F.A. Hayek, 1974 Economics Nobel Laureate\n\nThis course examines the dual role of political institutions and economic forces that determine the relative prosperity, conflict, and/or flourishing of human beings in different social settings. As political economy–the enduring tradition that Adam Smith and others developed before technical “economics” emerged in the 20th century–it deals with some of the moral and institutional problems that societies must grapple with, and why some succeed or fail more relative to others. While we consider the theory of various political and economic institutions, we often apply them to an international context of trade, globalization, foreign aid, conflict, development in our increasingly global-oriented cultures. As a result, a major goal of this course is understanding the state of the world and current events today, as well as being able to reason through political economic problems.\nThis is an economics course, and as such we use the tools of microeconomics to understand not just economic, but also political, social, and cultural issues. Basic familiarity with microeconomics is essential to understanding how decisions made by agents in different institutional contexts lead to social coordination and the creation of value (or fail to do so). While I would prefer that you have some background, it is possible to take the course with little to no understanding of formal economic theory. The required prerequisite courses are ECON 205, PCSI 215, OR GLBS 200.\nThis course will have been successful if you are able to comprehend and apply the tools and insights of political economy towards explaining differences in real world political, economic, and cultural settings and are able to analyze current events. Primarily, this means applying models of rational choices for individual and firm behavior in different social contexts, and understanding the role of market institutions of property rights, prices, and profit and losses in bringing about social coordination."
  },
  {
    "objectID": "courses/econ-304.html#materials",
    "href": "courses/econ-304.html#materials",
    "title": "ECON 304 — International Political Economy",
    "section": "Materials",
    "text": "Materials\n\n\n\nLecture Slides\n\n\n\n\n1. Introduction to IPE\n\n\n2. Review of Microeconomics\n\n\n3. Property and the Rule of Law\n\n\n4. Profit-Seeking and Rent-Seeking\n\n\n5. Moral Dilemmas of Markets\n\n\n6. Violence and Order\n\n\n7. Feudalism and the Natural State\n\n\n8. Mercantilism and the Natural State\n\n\n9. Socialism and National Socialism\n\n\n10. Liberal Democracy and Open-Access Orders"
  },
  {
    "objectID": "courses/ecmg-212.html",
    "href": "courses/ecmg-212.html",
    "title": "ECMG 212 — Statistics for Economics and Business",
    "section": "",
    "text": "[Spring 2017 Syllabus]\n\n“There are three kinds of lies: lies, damned lies, and statistics.” — Benjamin Disraeli, British P.M.\n\n\n“The sexy jobs in the next ten years will be statisticians.” — Hal Varian, Chief Economist, Google\n\nStatistics is the science of learning from data. Businesses, governments, academics, nonprofit organizations, consultants, and many other professions are using more data than ever. The rise of the internet and the ability for organizations to track many things about their users, known as “big data,” make statistical literacy and competency one of the most in-demand skills that most employers today are looking for.\nThis course is designed as an introduction to data and how it is collected, described, and used to make useful inferences about the world. I am an economist, not a business expert nor a mathematician. While we will be dealing with elementary statistical and probability theory, we will be keeping our eye towards applications in business and economics. If you want a more ``pure” class in statistics or mathematics, they are offered by the department of mathematics. The formal prerequisites for this course are MATH 120 or equivalent. I assume that you have no background in statistics or probability (we will start from square one), but that you are competent in basic algebra and graphing skills (we may do a brief review as necessary).\nI have three goals for everyone taking this course: (1) to understand and evaluate statistical and empirical claims; (2) to understand research design and hypothesis testing; (3) to gain experience working with, interpreting, and communicating real data. I am less concerned with forcing you to memorize and recite proofs of statistical estimator properties, and more concerned with the development of your intuitions and the ability to think critically as a businessperson, a social scientist, and a democratic citizen—although this will require you to demonstrate proficiency with some statistical and mathematical tools.\nTo these ends, in addition to lectures about statistical methods, you will be working on problem sets that use theory, as well as using Microsoft Excel to complete problem sets using data, and write a brief empirical paper using data. By the end, you should feel comfortable working with data and understanding the empirical claims of others. The best training is for you to learn by doing."
  },
  {
    "objectID": "courses/ecmg-212.html#overview",
    "href": "courses/ecmg-212.html#overview",
    "title": "ECMG 212 — Statistics for Economics and Business",
    "section": "",
    "text": "[Spring 2017 Syllabus]\n\n“There are three kinds of lies: lies, damned lies, and statistics.” — Benjamin Disraeli, British P.M.\n\n\n“The sexy jobs in the next ten years will be statisticians.” — Hal Varian, Chief Economist, Google\n\nStatistics is the science of learning from data. Businesses, governments, academics, nonprofit organizations, consultants, and many other professions are using more data than ever. The rise of the internet and the ability for organizations to track many things about their users, known as “big data,” make statistical literacy and competency one of the most in-demand skills that most employers today are looking for.\nThis course is designed as an introduction to data and how it is collected, described, and used to make useful inferences about the world. I am an economist, not a business expert nor a mathematician. While we will be dealing with elementary statistical and probability theory, we will be keeping our eye towards applications in business and economics. If you want a more ``pure” class in statistics or mathematics, they are offered by the department of mathematics. The formal prerequisites for this course are MATH 120 or equivalent. I assume that you have no background in statistics or probability (we will start from square one), but that you are competent in basic algebra and graphing skills (we may do a brief review as necessary).\nI have three goals for everyone taking this course: (1) to understand and evaluate statistical and empirical claims; (2) to understand research design and hypothesis testing; (3) to gain experience working with, interpreting, and communicating real data. I am less concerned with forcing you to memorize and recite proofs of statistical estimator properties, and more concerned with the development of your intuitions and the ability to think critically as a businessperson, a social scientist, and a democratic citizen—although this will require you to demonstrate proficiency with some statistical and mathematical tools.\nTo these ends, in addition to lectures about statistical methods, you will be working on problem sets that use theory, as well as using Microsoft Excel to complete problem sets using data, and write a brief empirical paper using data. By the end, you should feel comfortable working with data and understanding the empirical claims of others. The best training is for you to learn by doing."
  },
  {
    "objectID": "courses/ecmg-212.html#materials",
    "href": "courses/ecmg-212.html#materials",
    "title": "ECMG 212 — Statistics for Economics and Business",
    "section": "Materials",
    "text": "Materials\n\n\n\nLesson\nSlides\nExcel Templates\n\n\n\n\n1. Sampling and Data\nSlides\n\n\n\n2. Descriptive Statistics\nSlides\nDescriptive Statistics; Boxplots\n\n\n3. Probability\nSlides\n\n\n\n4. Discrete Random Variables\nSlides\n\n\n\n5. Continuous Random Variables\nSlides\nModelling Distributions\n\n\n6. The Normal Distribution\nSlides\n\n\n\n7. The Central Limit Theorem\nSlides\n\n\n\n8. Confidence Intervals\nSlides\nRegression Example\n\n\n9. Hypothesis Testing\nSlides\n\n\n\n10. Linear Regression\nSlides"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ryan Safner",
    "section": "",
    "text": "I am originally from Connecticut, I got my bachelor’s in Economics at UConn and my Ph.D in Economics at George Mason University in 2015. Between graduate school and my family’s careers, I have lived in 5 states (if you include the District of Columbia) and I currently live in Takoma Park, MD with my wife and kid.\nI’m a big fan of Miami Hurricanes college football (my wife’s undergrad alma mater) and UConn Huskies basketball. In my spare time, I enjoy running, cycling, reading, eating (and trying to cook) good (mostly vegetarian) food, traveling, and enjoying lots of live music with my wife.\nAs an economist, I am interested in understanding what institutions and policies best allow us to come up with inventions and ideas that benefit everyone. This has led me down rabbit holes ranging from 17th century English institutional change, to 19th century American literary piracy, to the success of Wikipedia, to crowdfunding, blockchains, & generative AI. Aspirationally, I am trying to expand my research and software skill horizons into more computational social science themes with agent-based modeling. You can find more about my research interests from a high-level, non-technical perspective at Increasing Returns.\nI consider myself a lifelong educator (& humble student) of economics and social science. Most of my post-graduate life was focused on the conveyor belt of academia, ultimately earning tenure at Hood College in 2022. Despite this achievement (or perhaps because of it), I decided I needed a change in perspective and wanted to experience life outside the ivory tower while still learning, researching, and contributing to the public good. Despite no longer being in the classroom with students (which I miss), I still enjoy keeping up with the scholarly literature (my wife tells me I read too much non-fiction) and writing about economic issues well beyond my day job.\nI am also an amateur tech nerd. With no formal training in computer science or software engineering, I have put disproportionate time into learning and practicing open source data science software. I enjoy tinkering around in R and producing useful visualizations and simulations, many of which I have posted on my software page. Except when absolutely required in my current job, I have not touched Microsoft Office since 2011, I write all of my research papers, teaching materials, even this website, in R and markdown (which I fully explain in my econometrics course). I very occasionally blog about these tools here.\nI enjoy meeting new people with different interests and backgrounds, so feel free to reach out to me by email or on LinkedIn!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n \n\n\n \n\n\n\n\nOct 18, 2019\n\n\nIf You’re Going to Learn R, Learn the Tidyverse\n\n\n\n\nAug 22, 2019\n\n\nNew Course Websites\n\n\n\n\nAug 6, 2019\n\n\nEconometrics, Data Science, and Causal Inference\n\n\n\n\nDec 28, 2018\n\n\nReplicating Stata’s ‘Robust’ Option for OLS Standard Errors in R\n\n\n\n\nOct 7, 2018\n\n\nVisualizing Tax Incidence with Shiny\n\n\n\n\nSep 28, 2018\n\n\nVisualizing Linear Regression with Shiny\n\n\n\n\nSep 8, 2018\n\n\nEconometrics Lecture Slides on GitHub\n\n\n\n\nSep 2, 2018\n\n\nTest Post Please Don’t Ignore\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "courses/econ-205.html",
    "href": "courses/econ-205.html",
    "title": "ECON 205 — Principles of Macroeconomics",
    "section": "",
    "text": "[Fall 2018 Syllabus]\n\nThe curious task of economics is to demonstrate to men how little they really know about what they imagine they can design. - F.A. Hayek, 1974 Economics Nobel Laureate\n\nThe goal of this course is to introduce you to the economic way of thinking and to demonstrate its power in comprehending the world around us. This class assumes you have no knowledge of economics whatsoever, and I teach it like it is the first, and last, economics class you will ever take. I hope it will not be.\nSo what is economics? Most people, upon hearing the words “economics” or “economist” imagine some combination of money, business, and those folks on the news who report “the latest numbers” on the economy (which always seem to be different than previous forecasts). These examples certainly might use economics, but economics really is a much broader way of thinking, analyzing, and understanding the world around us. With economics, you will learn to appreciate the role of prices, incentives, and institutions in fostering cooperation and social order to achieve all of the things (material and nonmaterial) that we desire from the human experience.\nThis course will have been successful if you are able to apply the economic way of thinking to your own passions, future studies, and professional aspirations that involve social, political, and economic issues. Understanding basic economics will also make you a better citizen in our democratic and market-based society. I invite you to diligently grapple with the ideas raised in our readings and discussions, and to do all that you can to try to understand and explore the world around you. More specifically, I aim for you to be able to:\n\nEffectively understand, articulate, and critique economic arguments verbally and in writing\nApply economic tools and theories to market conditions, including using appropriate graphical and mathematical tools.\nObtain basic understanding of economic statistics, as well as know where to collect them and how to effectively present them graphically and numerically.\nUnderstand current policy debates and formulate informed opinions"
  },
  {
    "objectID": "courses/econ-205.html#overview",
    "href": "courses/econ-205.html#overview",
    "title": "ECON 205 — Principles of Macroeconomics",
    "section": "",
    "text": "[Fall 2018 Syllabus]\n\nThe curious task of economics is to demonstrate to men how little they really know about what they imagine they can design. - F.A. Hayek, 1974 Economics Nobel Laureate\n\nThe goal of this course is to introduce you to the economic way of thinking and to demonstrate its power in comprehending the world around us. This class assumes you have no knowledge of economics whatsoever, and I teach it like it is the first, and last, economics class you will ever take. I hope it will not be.\nSo what is economics? Most people, upon hearing the words “economics” or “economist” imagine some combination of money, business, and those folks on the news who report “the latest numbers” on the economy (which always seem to be different than previous forecasts). These examples certainly might use economics, but economics really is a much broader way of thinking, analyzing, and understanding the world around us. With economics, you will learn to appreciate the role of prices, incentives, and institutions in fostering cooperation and social order to achieve all of the things (material and nonmaterial) that we desire from the human experience.\nThis course will have been successful if you are able to apply the economic way of thinking to your own passions, future studies, and professional aspirations that involve social, political, and economic issues. Understanding basic economics will also make you a better citizen in our democratic and market-based society. I invite you to diligently grapple with the ideas raised in our readings and discussions, and to do all that you can to try to understand and explore the world around you. More specifically, I aim for you to be able to:\n\nEffectively understand, articulate, and critique economic arguments verbally and in writing\nApply economic tools and theories to market conditions, including using appropriate graphical and mathematical tools.\nObtain basic understanding of economic statistics, as well as know where to collect them and how to effectively present them graphically and numerically.\nUnderstand current policy debates and formulate informed opinions"
  },
  {
    "objectID": "courses/econ-205.html#materials",
    "href": "courses/econ-205.html#materials",
    "title": "ECON 205 — Principles of Macroeconomics",
    "section": "Materials",
    "text": "Materials\n\n\n\nLesson Slides\nHandouts\nPractice Problems\nHomework\n\n\n\n\n1. The Economic Way of Thinking\n\n\nHW #1\n\n\n2. Cooperation and Exchange\n\nComparative Advantage Practice (Answers)\nHW #2\n\n\n3. Supply and Demand\n\nSupply and Demand Practice (Answers)\nHW #3\n\n\n4. Markets and Economic Policy\n\n\nHomework #4\n\n\n5. Measuring the Macroeconomy\n\n\nHomework #5\n\n\n6. Economic Growth\n\n\n\n\n\n7. Money and the Financial System\n\n\nHomework #6\n\n\n8. Monetary and Fiscal Policy"
  },
  {
    "objectID": "courses/index.html",
    "href": "courses/index.html",
    "title": "Courses",
    "section": "",
    "text": "These are course materials for courses I have taught that do not have their own dedicated course website. This means I haven’t taught them since Fall 2019, when I started making course websites."
  },
  {
    "objectID": "posts/econometrics-slides-on-github.html",
    "href": "posts/econometrics-slides-on-github.html",
    "title": "Econometrics Lecture Slides on GitHub",
    "section": "",
    "text": "This semester, I am posting my lecture slides for my Econometrics class on GitHub. While I usually post my lecture slides for my courses on my website or link to my Dropbox, those are the final PDF documents. On GitHub, I am posting both the final PDFs as well as the source .rmd files. I am primarily doing this for my econometrics students, who will be learning R and R Markdown for their assignments (which is how I write my slides), but this is also open to anyone.\nThis brings two benefits. First, it provides an example of how to use R Markdown and the benefits of writing plain-text files to convert into PDFs, html and other outputs (more on that in later posts). Second, it is also an example of using GitHub as a version control system for maintaining backups, a platform for collaboration, and exposure to industry tools of the software trade.\nOver the summer, I also converted to writing my research papers in R Markdown and managing them on GitHub (and the same with this website). I have also attempted to begin collaborating with some of my coauthors via GitHub. They are presently in private repositories to keep research private until it is ready for publication, but I may consider making more public as examples.\nI have updated the Readme file on GitHub to provide more information for how to view, download, and read the .rmd source files used to generate the slides. In future posts, I will discuss more about version control with GitHub, writing with R Markdown, and managing workflow with these tools."
  },
  {
    "objectID": "posts/new-course-websites.html",
    "href": "posts/new-course-websites.html",
    "title": "New Course Websites",
    "section": "",
    "text": "This Fall semester, I have made dedicated websites for all of my courses at Hood College that host nearly all the course content. You can see them all here. My interest was sparked when I saw Andrew Heiss’ amazing course websites.\nUntil this point, all of my course content has lived on Blackboard for my students, though I have also tried to post syllabi and lecture slides (if not additional resources) on my personal website over the past few years. This process has been manageable, but has had way too much pointing-and-clicking, unnecessary duplication, and is not very open-source.\nFor each course website, I am using the (surprisingly painless) combination of R markdown, GitHub, Blogdown, Hugo, and Netlify such that my entire workflow is reduced to:\nStep three doesn’t even really count because once set up, I don’t have to do anything! I paid the fixed costs of extensively figuring out how everything works and experimenting over this past summer. If you are interested, I have attempted to explain the process of duplicating everything in this README file in my testing repository on GitHub (automatically rendered on the main page). It assumes some working knowledge of R Markdown, but not much else. One day I may make a better step-by-step guide if there is any interest, but lots of great guides on each feature already exist (and are linked to in the README).\nEach course has a page for the syllabus, assignments, reference (guides, links, and other resources to come), and a schedule page that is organized into relevant slides, assignments, or other resources for each class meeting. I have found the schedule page in particular to be a great framework for making all aspects of the course cohere in a single place.\nWhy have I decided to do this?\nFor good or ill, I am a slave to aesthetics1, and think that form matters just as much as function. This is not so much a rational viewpoint of mine as a vice: Producing and perfecting lectures is a consumption good for me, not a means-to-an-end investment. I have delighted at learning Xaringan this summer for all of my slides (guides and blog posts coming soon), and as they live in html, hosting them on a single website made the most sense. I also made sure each course has its own custom hex sticker and workflow map (again, copying some killer design elements from Heiss). As a relevant side note, I have started to see the limitations and ugliness of PDFs in the age of the web, and have ditched LaTeX (admittedly, it was a long and loving relationship) for the elegance and beauty of markdown and html wherever possible (posts on that to come).\nThe main side effect is that all of my materials will be available to anyone online for free. I have always been committed to free and open source teaching, and have benefitted an extraordinary amount from learning from materials that other academics or practitioners - who I may never meet - have posted online.2 While they will be without lecture videos (no plans for that, sorry), or original copies of some readings (copyright, my favorite), or answer keys for major assignments (I do recycle some questions), these websites are otherwise complete self-contained courses.\nAnother reason this made sense for me is that as I have gotten into a rhythm of teaching the same course multiple times, I have started to accumulate a lot of additional resources: guides, tutorials, handouts on more technical concepts, etc. These would kill a lot of trees to print out for students, and it would be nice to have a central repository for them - a website is the natural place.\nThere are some limitations, as I can’t do everything on them. My major limitation is grading. Blackboard’s Grading Center is too convenient and centralized not to use (not to mention, secure, individualized, and FERPA-compliant). These new websites are primarily for broadcasting content to everyone, not using it as a personalized experience. Additionally, I will still use Blackboard for mass-emailing students. But my workflow of 1. write in plain text, 2. push to GitHub available for everyone seems vastly superior to endless mouse clicks and writing of individual items to a page viewable by a single course section (and then duplicating everything for a second section).\nIt remains to be seen how my students will recieve these, but for now, I am excited."
  },
  {
    "objectID": "posts/new-course-websites.html#footnotes",
    "href": "posts/new-course-websites.html#footnotes",
    "title": "New Course Websites",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI view lecture slides as a fine artform and have refused to use Microsoft Office products for over 8 years↩︎\nOne of my guilty pleasures is to devour anyone’s lecture slides posted free online in areas that I teach or research and shamelessly steal what I like (I try to give credit if it is something major!). This confirms to me that I am a true academic.↩︎"
  },
  {
    "objectID": "posts/test-post-please-dont-ignore.html",
    "href": "posts/test-post-please-dont-ignore.html",
    "title": "Test Post Please Don’t Ignore",
    "section": "",
    "text": "With the new school year in full swing, I have redesigned my personal website, and plan to make occasional posts on the tools I use in my research and teaching. Over the summer, I made the full conversion to using R, R Markdown, and Github for nearly everything I do in my professional life (including managing my website with Hugo/Academic).\nThis semester, I am teaching econometrics to my students using R for the first time, and optionally nudging them to use R Markdown for their homeworks and paper assignment. My training, both in undergraduate and graduate school, was with Stata, so I am particularly excited to shake things up a bit.\nI primarily see this blog as the venue to occasionally post about my experiences and as well as tutorials in using these tools for research and teaching. I have benefited enormously in the past year thanks to people like Kieran Healey, Jenny Bryan, Hadley Wickham, Yihue Xie, Jake Russ, Steven Miller, J. Alexander Branham and Vlad Tarko. Most of these are academics that I have never met, but they were generous enough to make syllabi, lecture slides, source files, and blog posts public on their websites and on Github.\nI hope to do my small part to spread word about these useful tools and post examples I use in class or in my research here."
  },
  {
    "objectID": "posts/visualizing-tax-incidence-with-shiny.html",
    "href": "posts/visualizing-tax-incidence-with-shiny.html",
    "title": "Visualizing Tax Incidence with Shiny",
    "section": "",
    "text": "With the new school year in full swing, I have redesigned my personal website, and plan to make occasional posts on the tools I use in my research and teaching. Over the summer, I made the full conversion to using R, R Markdown, and Github for nearly everything I do in my professional life (including managing my website with Hugo/Academic).\nThis semester, I am teaching econometrics to my students using R for the first time, and optionally nudging them to use R Markdown for their homeworks and paper assignment. My training, both in undergraduate and graduate school, was with Stata, so I am particularly excited to shake things up a bit.\nI primarily see this blog as the venue to occasionally post about my experiences and as well as tutorials in using these tools for research and teaching. I have benefited enormously in the past year thanks to people like Kieran Healey, Jenny Bryan, Hadley Wickham, Yihue Xie, Jake Russ, Steven Miller, J. Alexander Branham and Vlad Tarko. Most of these are academics that I have never met, but they were generous enough to make syllabi, lecture slides, source files, and blog posts public on their websites and on Github.\nI hope to do my small part to spread word about these useful tools and post examples I use in class or in my research here."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "This page is under construction, but most of my projects are open source and can be found in my GitHub repositories. This includes all of the materials for my course websites and course content."
  },
  {
    "objectID": "software.html#shiny-portfolio",
    "href": "software.html#shiny-portfolio",
    "title": "Software",
    "section": "Shiny Portfolio",
    "text": "Shiny Portfolio\nThese are largely R shiny apps that I have written to demonstrate economic concepts for my economics courses. The main purpose is to visualize concepts in action (often through a graph) and especially show how key parameters affect the outcome by allowing users to change these values and see the graph update dynamically.\n\nConsumer Theory\n\n\n\n\nConsumer’s Problem\n\nChanges in Consumer’s Problem\n\n\n\n\n\nIndustrial Organization\n\n\n\n\nProfit Maximization with Market Power\n\nCournot Oligopoly with Market Changes\n\n\nCournot Oligopoly with \\(N\\) Firms\n\nCournot Oligopoly with Asymmetric Costs\n\n\n\n\n\nGame Theory\n\n\n\nRepeated Prisoners’ Dilemma\n\nEvolutionary Hawk Dove Game\n\n\n\n\n\nMicroeconomics/Misc.\n\n\n\n\nElasticity and Surpluses\nTax Incidence\n\n\n\nCalculus of Consent Model\nOrdinary Least Squares Regression\n\n\n\nSolow Growth Model\n\n\n\n\n\nGrade Calculator & Forecaster (for ECON 306; with variants for each of courses, found on respective course websites)"
  },
  {
    "objectID": "software.html#economics-graphs-in-ggplot",
    "href": "software.html#economics-graphs-in-ggplot",
    "title": "Software",
    "section": "Economics Graphs in ggplot",
    "text": "Economics Graphs in ggplot\n[IN PROGRESS]\nI have taught a variety of economics courses and make all of my slides and course materials in R/Xaringan/Quarto. This includes a very wide variety of graphs and illustrations of economic concepts, all of which I have drawn using ggplot2. I will begin creating a library of different graphs and the source code to make them (and one day make a package with dedicated functions to easily make each of them).\nUntil I have that ready, it’s best just to poke around the slides for all of my courses."
  },
  {
    "objectID": "software.html#course-websites",
    "href": "software.html#course-websites",
    "title": "Software",
    "section": "Course Websites",
    "text": "Course Websites\nSince the Fall of 2019, I have made individualized websites for each of my courses. You can find links to them on my teaching page, and the repositories on my GitHub."
  },
  {
    "objectID": "software.html#hex-stickers",
    "href": "software.html#hex-stickers",
    "title": "Software",
    "section": "Hex Stickers",
    "text": "Hex Stickers\nAs part of my themeing for each of my courses, I have been making a custom hex stickers in the style that has become popular in the R/tidyverse community. You can find the image sources, outputs, and the code to render each one in this repository."
  }
]